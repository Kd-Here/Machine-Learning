{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvYDy9apGRlt"
   },
   "source": [
    "# Optional Lab: Diagnosing Bias and Variance\n",
    "\n",
    "In the previous optional lab, you saw how to evaluate a learning algorithm's performance by measuring its training and cross validation error. Given these values, you are able to quantify how well a model is doing and this helps you make a decision on which one to use for a given application. In this lab, you will build upon this process and explore some tips to improve the performance of your models. As it turns out, the training and cross validation errors can tell you what to try next to improve your models. Specifically, it will show if you have a high bias (underfitting) or high variance (overfitting) problem. This lecture slide shown below gives an example:\n",
    "\n",
    "<img src='images/C2_W3_BiasVariance.png' width=75%>\n",
    "\n",
    "The leftmost figure shows a high bias problem where the model is not capturing the patterns in the training data. As a result, you will have a high training and cross validation error. The rightmost figure, on the other hand, shows a high variance problem where the model has overfit the training set. Thus, even though it has a low training error, it will perform poorly on new examples. That is indicated by a high cross validation error. The ideal model would be the figure in the middle, where it successfully learns from the training set and also generalizes well to unseen data. The lectures gave some tips on what to do next to achieve this \"just right\" model. \n",
    "\n",
    "To fix a high bias problem, you can:\n",
    "* try adding polynomial features\n",
    "* try getting additional features\n",
    "* try decreasing the regularization parameter\n",
    "\n",
    "To fix a high variance problem, you can:\n",
    "* try increasing the regularization parameter\n",
    "* try smaller sets of features\n",
    "* get more training examples\n",
    "\n",
    "You will try all these tips in this lab. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing Baseline Level of Performance\n",
    "\n",
    "Before you can diagnose a model for high bias or high variance, it is usually helpful to first have an idea of what level of error you can reasonably get to. As mentioned in class, you can use any of the following to set a baseline level of performance.\n",
    "\n",
    "* human level performance\n",
    "* competing algorithm's performance\n",
    "* guess based on experience\n",
    "\n",
    "Real-world data can be very noisy and it's often infeasible to get to 0% error. For example, you might think that you have a high bias problem because you're getting 10% training and 15% cross validation error on a computer vision application. However, you later found out that even humans can't perform better than 10% error. If you consider this the baseline level, then you now instead have a high variance problem because you've prioritized minimizing the gap between cross validation and training error.\n",
    "\n",
    "With this in mind, let's begin exploring the techniques to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj1P1kTmGRlv"
   },
   "source": [
    "## Imports and Lab Setup\n",
    "\n",
    "Aside from a couple of [linear regressors](https://scikit-learn.org/stable/modules/classes.html#classical-linear-regressors) from scikit-learn, all other functions used in this lab are found in the `utils.py` file outside this notebook. You will mostly use the same code as the last lab so you don't need to see each line here again. It mostly contains functions to split your data, as well as functions that loop over a list of parameters (e.g. degree of polynomial, regularization parameter) and plots the training and cross validation error for each one. Feel free to explore the code in the said file to see the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xuHD2EOxGRlv"
   },
   "outputs": [],
   "source": [
    "# for building linear regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# import lab utility functions in utils.py\n",
    "import utils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing High Bias\n",
    "\n",
    "You will first look at things to try when your model is underfitting. In other words, when the training error is far worse than the baseline level of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try adding polynomial features\n",
    "\n",
    "You've already seen this in the previous lab. Adding polynomial features can help your model learn more complex patterns in your data. Here again is an example of a plot showing how the training and cross validation errors change as you add more polynomial features. You will be using a synthetic dataset for a regression problem with one feature and one target. In addition, you will also define an arbitrary baseline performance and include it in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, cv, and test\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data1.csv')\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training inputs (1 feature):\\n {x_train[:5]}\\n\")\n",
    "\n",
    "# Instantiate the regression model class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train and plot polynomial regression models\n",
    "utils.train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the more polynomial features you add, the better the model fits to the training data. In this example, it even performed better than the baseline. At this point, you can say that the models with degree greater than 4 are low-bias because they perform close to or better than the baseline.\n",
    "\n",
    "However, if the baseline is defined lower (e.g. you consulted an expert regarding the acceptable error), then the models are still considered high bias. You can then try other methods to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and plot polynomial regression models. Bias is defined lower.\n",
    "utils.train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try getting additional features\n",
    "\n",
    "Another thing you can try is to acquire other features. Let's say that after you got the results above, you decided to launch another data collection campaign that captures another feature. Your dataset will now have 2 columns for the input features as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data2.csv')\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training inputs (2 features):\\n {x_train[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see what this does to the same training process as before. You'll notice that the training error is now closer to (or even better than) the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train and plot polynomial regression models. Dataset used has two features.\n",
    "utils.train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=6, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try decreasing the regularization parameter\n",
    "\n",
    "At this point, you might want to introduce regularization to avoid overfitting. One thing to watch out for is you might make your models underfit if you set the regularization parameter too high. The cell below trains a 4th degree polynomial model using the [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) class which allows you to set a regularization parameter (i.e. lambda or $\\lambda$). You will try several values and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lambdas to plot\n",
    "reg_params = [10, 5, 2, 1, 0.5, 0.2, 0.1]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "utils.train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot shows an initial $\\lambda$ of `10` and as you can see, the training error is worse than the baseline at that point. This implies that it is placing a huge penalty on the `w` parameters and this prevents the model from learning more complex patterns in your data. As you decrease $\\lambda$, the model loosens this restriction and the training error is able to approach the baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing High Variance\n",
    "\n",
    "You will now look at some things to try when your model has overfit the training set. The main objective is to have a model that generalizes well to new examples so you want to minimize the cross validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try increasing the regularization parameter\n",
    "\n",
    "In contrast to the last exercise above, setting a very small value of the regularization parameter will keep the model low bias but might not do much to improve the variance. As shown below, you can improve your cross validation error by increasing the value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lambdas to plot\n",
    "reg_params = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "utils.train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try smaller sets of features\n",
    "\n",
    "You've already seen in the last lab that having too many polynomial terms can result in overfitting. You can reduce the number of such terms and see where you get the best balance of training and cross validation error. Another scenario where reducing the number of features would be helpful is when you have irrelevant features in your data. For example, patient IDs that hospitals give will not help in diagnosing a tumor so you should make sure to remove it from your training data. \n",
    "\n",
    "To illustrate how removing features can improve performance, you will do polynomial regression for 2 datasets: the same data you used above (2 features) and another with a random ID column (3 features). You can preview these using the cell below. Notice that 2 columns are identical and a 3rd one is added to include random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset with randomID feature\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data2.csv')\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training set with 2 features:\\n {x_train[:5]}\\n\")\n",
    "\n",
    "# Prepare dataset with randomID feature\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data3.csv')\n",
    "\n",
    "# Preview the first 5 rows\n",
    "print(f\"first 5 rows of the training set with 3 features (1st column is a random ID):\\n {x_train[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will train the models and plot the results. The solid lines in the plot show the errors for the data with 2 features while the dotted lines show the errors for the dataset with 3 features. As you can see, the one with 3 features has higher cross validation error especially as you introduce more polynomial terms. This is because the model is also trying to learn from the random IDs even though it has nothing to do with the target. \n",
    "\n",
    "Another way to look at it is to observe the points at degree=4. You'll notice that even though the *training error* is lower with 3 features, the *gap between the training error and cross validation error* is a lot wider than when you only use 2 features. This should also warn you that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define properties of the 2 datasets\n",
    "file1 = {'filename':'data/c2w3_lab2_data3.csv', 'label': '3 features', 'linestyle': 'dotted'}\n",
    "file2 = {'filename':'data/c2w3_lab2_data2.csv', 'label': '2 features', 'linestyle': 'solid'}\n",
    "files = [file1, file2]\n",
    "\n",
    "# Train and plot for each dataset\n",
    "utils.train_plot_diff_datasets(model, files, max_degree=4, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more training examples\n",
    "\n",
    "Lastly, you can try to minimize the cross validation error by getting more examples. In the cell below, you will train a 4th degree polynomial model then plot the *learning curve* of your model to see how the errors behave when you get more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "x_train, y_train, x_cv, y_cv, x_test, y_test = utils.prepare_dataset('data/c2w3_lab2_data4.csv')\n",
    "print(f\"the shape of the entire training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the entire training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the entire cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the entire cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "\n",
    "# Instantiate the model class\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define the degree of polynomial and train the model using subsets of the dataset.\n",
    "utils.train_plot_learning_curve(model, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, it shows that the cross validation error starts to approach the training error as you increase the dataset size. Another insight you can get from this is that adding more examples will not likely solve a high bias problem. That's because the training error remains relatively flat even as the dataset increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this lab, you were able to practice how to address high bias and high variance in your learning algorithm. By learning how to spot these issues, you have honed your intuition on what to try next when developing your machine learning models. In the next lectures, you will look deeper into the machine learning development process and explore more aspects that you need to take into account when working on your projects. See you there!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
